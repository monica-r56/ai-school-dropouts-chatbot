{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmEbDy6TsqSy",
        "outputId": "bbeb670c-e688-4771-a7bb-0894423f400f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasa\n",
            "  Downloading rasa-1.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting rasa-sdk\n",
            "  Downloading rasa_sdk-3.11.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting PyJWT<1.8,>=1.7 (from rasa)\n",
            "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting SQLAlchemy<1.4.0,>=1.3.3 (from rasa)\n",
            "  Downloading SQLAlchemy-1.3.24.tar.gz (6.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting absl-py<0.10,>=0.9 (from rasa)\n",
            "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install rasa rasa-sdk transformers torch nltk googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwIHchxcssFA",
        "outputId": "3fa596fb-cea8-464a-f665-69741a8073c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Using cached googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Using cached httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Using cached hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=ee704ba1db9925624cd7a2a83eeafa66a8cd354fdb6dc77872193b5475b806de\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.3.11 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.61.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import pandas as pd\n",
        "from googletrans import Translator"
      ],
      "metadata": {
        "id": "JC1ZaFLrsuqf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "df = pd.read_csv(\"school_dropout.csv\")\n",
        "#NLP mode\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "#embeddings\n",
        "dropout_reasons = df[\"User\"].tolist()\n",
        "reason_embeddings = model.encode(dropout_reasons, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "RzMfGZsMszCS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#translator\n",
        "translator = Translator()"
      ],
      "metadata": {
        "id": "ECfVBZuCtM4W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supported_languages = {\n",
        "    \"en\": \"English\",\n",
        "    \"ta\": \"à®¤à®®à®¿à®´à¯ (Tamil)\"\n",
        "}"
      ],
      "metadata": {
        "id": "gvXnPLfEtPqp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#closest matching dropout reason\n",
        "def match_reason(user_input):\n",
        "    input_embedding = model.encode(user_input, convert_to_tensor=True)\n",
        "    similarities = util.pytorch_cos_sim(input_embedding, reason_embeddings)\n",
        "    best_match_idx = torch.argmax(similarities).item()\n",
        "    return dropout_reasons[best_match_idx]"
      ],
      "metadata": {
        "id": "JXmVcxQStZIX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#solution\n",
        "def get_solution(user_input, language=\"en\"):\n",
        "    matched_reason = match_reason(user_input)\n",
        "    solution = df[df[\"User\"] == matched_reason][\"Assistant\"].values\n",
        "    response = solution[0] if len(solution) > 0 else \"Sorry, no solution available.\"\n",
        "    #translate the response\n",
        "    if language == \"ta\":\n",
        "        response = translator.translate(response, src=\"en\", dest=\"ta\").text\n",
        "    return response"
      ],
      "metadata": {
        "id": "--gAy6NmtbfJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot interaction\n",
        "def chatbot_interact():\n",
        "    print(\"\\nğŸ¤– Hello! I'm here to help you continue your education. Let's get started! ğŸ˜Š\")\n",
        "    #language options\n",
        "    print(\"\\nğŸŒ Choose your preferred language:\")\n",
        "    for lang_code, lang_name in supported_languages.items():\n",
        "        print(f\"  [{lang_code}] {lang_name}\")\n",
        "    #preferred language\n",
        "    while True:\n",
        "        language = input(\"\\nType the code of your preferred language (e.g., 'ta' for Tamil, 'en' for English): \").strip().lower()\n",
        "        if language in supported_languages:\n",
        "            break\n",
        "        print(\"âš ï¸ Invalid choice! Please enter a valid language code from the list.\")\n",
        "    print(f\"\\nğŸ¤– {translator.translate('Welcome! Letâ€™s start.', src='en', dest=language).text} ğŸ˜Š\")\n",
        "    #user details\n",
        "    name = input(translator.translate(\"What is your name? \", src=\"en\", dest=language).text)\n",
        "    age = input(translator.translate(\"\\nHow old are you? \", src=\"en\", dest=language).text)\n",
        "    place = input(translator.translate(\"\\nWhere are you from? \", src=\"en\", dest=language).text)\n",
        "    education_level = input(translator.translate(\"\\nTill which standard have you studied? \", src=\"en\", dest=language).text)\n",
        "    greeting = translator.translate(f\"\\nNice to meet you, {name}! Let's talk about your education journey. ğŸ˜Š\", src=\"en\", dest=language).text\n",
        "    print(greeting)\n",
        "    #dropout reason\n",
        "    while True:\n",
        "        dropout_reason = input(translator.translate(\"\\nWhy did you drop out of school? (Type 'exit' to stop) \", src=\"en\", dest=language).text)\n",
        "        if dropout_reason.lower() == \"exit\":\n",
        "            goodbye_msg = translator.translate(\"\\nGoodbye! Stay positive and keep learning. Reach out anytime! ğŸ˜Š\", src=\"en\", dest=language).text\n",
        "            print(goodbye_msg)\n",
        "            break\n",
        "        #selected language\n",
        "        response = get_solution(dropout_reason, language)\n",
        "        #chatbot response\n",
        "        response_msg = translator.translate(\"\\nBased on your reason, I suggest: \", src=\"en\", dest=language).text + response\n",
        "        print(response_msg)\n",
        "        #help\n",
        "        follow_up = input(translator.translate(\"\\nWould you like to explore more options? (yes/no) \", src=\"en\", dest=language).text)\n",
        "        if follow_up.lower() not in [\"yes\", translator.translate(\"yes\", src=\"en\", dest=language).text.lower()]:\n",
        "            farewell = translator.translate(\"\\nNo problem! Keep striving for success. Goodbye! ğŸ˜Š\", src=\"en\", dest=language).text\n",
        "            print(farewell)\n",
        "            break\n",
        "chatbot_interact()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sw4G3KMtz9H",
        "outputId": "6e7fb116-c51f-4d51-92d3-2b5f6fa8a682"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¤– Hello! I'm here to help you continue your education. Let's get started! ğŸ˜Š\n",
            "\n",
            "ğŸŒ Choose your preferred language:\n",
            "  [en] English\n",
            "  [ta] à®¤à®®à®¿à®´à¯ (Tamil)\n",
            "\n",
            "Type the code of your preferred language (e.g., 'ta' for Tamil, 'en' for English): ta\n",
            "\n",
            "ğŸ¤– à®µà®°à®µà¯‡à®±à¯à®•à®¿à®±à¯‹à®®à¯!à®†à®°à®®à¯à®ªà®¿à®•à¯à®•à®²à®¾à®®à¯. ğŸ˜Š\n",
            "à®‰à®™à¯à®•à®³à¯ à®ªà¯†à®¯à®°à¯ à®à®©à¯à®©?Anu\n",
            "à®‰à®™à¯à®•à®³à¯à®•à¯à®•à¯ à®à®µà¯à®µà®³à®µà¯ à®µà®¯à®¤à¯?16\n",
            "à®¨à¯€à®™à¯à®•à®³à¯ à®à®™à¯à®•à®¿à®°à¯à®¨à¯à®¤à¯ à®µà®°à¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯?Coimbatore\n",
            "à®à®¨à¯à®¤ à®¤à®°à®¤à¯à®¤à¯ˆ à®¨à¯€à®™à¯à®•à®³à¯ à®ªà®Ÿà®¿à®¤à¯à®¤à¯€à®°à¯à®•à®³à¯?10\n",
            "à®‰à®™à¯à®•à®³à¯ˆ à®šà®¨à¯à®¤à®¿à®¤à¯à®¤à®¤à®¿à®²à¯ à®®à®•à®¿à®´à¯à®šà¯à®šà®¿, à®…à®©à¯!à®‰à®™à¯à®•à®³à¯ à®•à®²à¯à®µà®¿ à®ªà®¯à®£à®®à¯ à®ªà®±à¯à®±à®¿ à®ªà¯‡à®šà®²à®¾à®®à¯..\n",
            "à®¨à¯€à®™à¯à®•à®³à¯ à®à®©à¯ à®ªà®³à¯à®³à®¿à®¯à¯ˆ à®µà®¿à®Ÿà¯à®Ÿà¯ à®µà¯†à®³à®¿à®¯à¯‡à®±à®¿à®©à¯€à®°à¯à®•à®³à¯?(à®¨à®¿à®±à¯à®¤à¯à®¤ 'à®µà¯†à®³à®¿à®¯à¯‡à®±à¯' à®à®©à¯à®±à¯ à®¤à®Ÿà¯à®Ÿà®šà¯à®šà¯ à®šà¯†à®¯à¯à®•)I can't afford to study\n",
            "à®‰à®™à¯à®•à®³à¯ à®•à®¾à®°à®£à®¤à¯à®¤à®¿à®©à¯ à®…à®Ÿà®¿à®ªà¯à®ªà®Ÿà¯ˆà®¯à®¿à®²à¯, à®¨à®¾à®©à¯ à®ªà®°à®¿à®¨à¯à®¤à¯à®°à¯ˆà®•à¯à®•à®¿à®±à¯‡à®©à¯:à®¤à®®à®¿à®´à¯à®¨à®¾à®Ÿà¯ à®®à®±à¯à®±à¯à®®à¯ à®•à¯‡à®°à®³à®¾ à®ªà¯‹à®©à¯à®± à®šà®¿à®² à®®à®¾à®¨à®¿à®²à®™à¯à®•à®³à¯ à®ˆ.à®Ÿà®ªà®¿à®³à¯à®¯à¯‚.à®à®¸à¯ à®®à®¾à®£à®µà®°à¯à®•à®³à¯à®•à¯à®•à¯ à®‡à®²à®µà®š à®•à®²à¯à®²à¯‚à®°à®¿à®•à¯ à®•à®²à¯à®µà®¿à®¯à¯ˆ à®µà®´à®™à¯à®•à¯à®•à®¿à®©à¯à®±à®©.\n",
            "à®®à¯‡à®²à¯à®®à¯ à®µà®¿à®°à¯à®ªà¯à®ªà®™à¯à®•à®³à¯ˆ à®†à®°à®¾à®¯ à®µà®¿à®°à¯à®®à¯à®ªà¯à®•à®¿à®±à¯€à®°à¯à®•à®³à®¾?(à®†à®®à¯/à®‡à®²à¯à®²à¯ˆ)exit\n",
            "à®à®¨à¯à®¤ à®ªà®¿à®°à®šà¯à®šà®©à¯ˆà®¯à¯à®®à¯ à®‡à®²à¯à®²à¯ˆ!à®µà¯†à®±à¯à®±à®¿à®•à¯à®•à¯ à®®à¯à®¯à®±à¯à®šà®¿ à®šà¯†à®¯à¯à®¯à¯à®™à¯à®•à®³à¯.à®•à¯à®Ÿà¯à®ªà¯ˆ!.\n"
          ]
        }
      ]
    }
  ]
}